{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Serena Yang\n",
    "### DSC 478 Homework 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/serenayang/Desktop/DSC478/Homework 4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this problem you will use a modified version of the item-based recommender algorithm from Ch. 14 of Machine Learning in Action and use it on joke ratings data based on Jester Online Joke Recommender System. The modified version of the code is provided in the module itemBasedRec.py. Most of the module will be used as is, but you will add some additional functionality.\n",
    "\n",
    "### The data set contains two files. The file \"modified_jester_data.csv\" contains the ratings on 100 jokes by 1000 users (each row is a user profile). The ratings have been normalized to be between 1 and 21 (a 20-point scale), with 1 being the lowest rating. A zero indicated a missing rating. The file \"jokes.csv\" contains the joke ids mapped to the actual text of the jokes.\n",
    "\n",
    "### Your tasks in this problem are the following (please also see comments for the function stubs in the provided module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "from numpy import linalg as la\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Load in the joke ratings data and the joke text data into appropriate data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load joke ratings data\n",
    "#genfromtxt can use on missing value\n",
    "ratings = genfromtxt(path + '/jokes/modified_jester_data.csv', delimiter = ',')\n",
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.18, 19.79,  1.34, ...,  0.  ,  0.  ,  0.  ],\n",
       "       [15.08, 10.71, 17.36, ..., 11.34,  6.68, 12.07],\n",
       "       [ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.  ,  0.  ],\n",
       "       ...,\n",
       "       [16.58, 16.63, 15.85, ...,  0.  ,  0.  ,  0.  ],\n",
       "       [ 3.67,  4.45,  3.67, ...,  3.77,  3.77,  3.28],\n",
       "       [ 9.88, 11.73,  9.16, ...,  0.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load joke text data\n",
    "def load_jokes(file):\n",
    "    jokes = genfromtxt(file, delimiter=',', dtype=str)\n",
    "    jokes = np.array(jokes[:,1])\n",
    "    return jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_joke_text(jokes, id):\n",
    "    return jokes[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes = load_jokes(path + '/jokes/jokes.csv')\n",
    "jokes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This couple had an excellent relationship going until one day he came home from work to find his girlfriend packing. He asked her why she was leaving him and she told him that she had heard awful things about him. \"What could they possibly have said to make you move out?\" \"They told me that you were a pedophile.\" He replied \"That\\'s an awfully big word for a ten year old.\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import functions from itemBasedRec.py\n",
    "def ecludSim(inA,inB):\n",
    "    return 1.0 / (1.0 + la.norm(inA - inB))\n",
    "\n",
    "def pearsSim(inA,inB):\n",
    "    if len(inA) < 3 : return 1.0\n",
    "    return 0.5 + 0.5 * corrcoef(inA, inB, rowvar = 0)[0][1]\n",
    "\n",
    "def cosSim(inA,inB):\n",
    "    num = float(inA.T * inB)\n",
    "    denom = la.norm(inA)*la.norm(inB)\n",
    "    return 0.5 + 0.5 * (num / denom)\n",
    "\n",
    "def standEst(dataMat, user, simMeas, item):\n",
    "    n = shape(dataMat)[1]\n",
    "    simTotal = 0.0; ratSimTotal = 0.0\n",
    "    for j in range(n):\n",
    "        userRating = dataMat[user,j]\n",
    "        if userRating == 0: continue\n",
    "        overLap = nonzero(logical_and(dataMat[:,item]>0, \\\n",
    "                                      dataMat[:,j]>0))[0]\n",
    "        if len(overLap) == 0: similarity = 0\n",
    "        else: similarity = simMeas(dataMat[overLap,item], \\\n",
    "                                   dataMat[overLap,j])\n",
    "        #print 'the %d and %d similarity is: %f' % (item, j, similarity)\n",
    "        simTotal += similarity\n",
    "        ratSimTotal += similarity * userRating\n",
    "    if simTotal == 0: return 0\n",
    "    else: return ratSimTotal/simTotal\n",
    "    \n",
    "def svdEst(dataMat, user, simMeas, item):\n",
    "    n = shape(dataMat)[1]\n",
    "    simTotal = 0.0; ratSimTotal = 0.0\n",
    "    data=mat(dataMat)\n",
    "    U,Sigma,VT = la.svd(data)\n",
    "    Sig4 = mat(eye(4)*Sigma[:4]) #arrange Sig4 into a diagonal matrix\n",
    "    xformedItems = data.T * U[:,:4] * Sig4.I  #create transformed items\n",
    "    for j in range(n):\n",
    "        userRating = data[user,j]\n",
    "        if userRating == 0 or j==item: continue\n",
    "        similarity = simMeas(xformedItems[item,:].T,\\\n",
    "                             xformedItems[j,:].T)\n",
    "        #print 'the %d and %d similarity is: %f' % (item, j, similarity)\n",
    "        simTotal += similarity\n",
    "        ratSimTotal += similarity * userRating\n",
    "    if simTotal == 0: return 0\n",
    "    else: return ratSimTotal/simTotal\n",
    "\n",
    "# This function is not needed for Assignment 4, but may be useful for experimentation\n",
    "def recommend(dataMat, user, N=3, simMeas=cosSim, estMethod=standEst):\n",
    "    unratedItems = nonzero(dataMat[user,:].A==0)[1] #find unrated items \n",
    "    if len(unratedItems) == 0: return 'you rated everything'\n",
    "    itemScores = []\n",
    "    for item in unratedItems:\n",
    "        estimatedScore = estMethod(dataMat, user, simMeas, item)\n",
    "        itemScores.append((item, estimatedScore))\n",
    "    return sorted(itemScores, key=lambda jj: jj[1], reverse=True)[:N]\n",
    "\n",
    "# This function performs evaluatoin on a single user based on the test_ratio\n",
    "# For example, with test_ratio = 0.2, a randomly selected 20 percent of rated \n",
    "# items by the user are withheld and the rest are used to estimate the withheld ratings\n",
    "\n",
    "def cross_validate_user(dataMat, user, test_ratio, estMethod=standEst, simMeas=pearsSim):\n",
    "    number_of_items = np.shape(dataMat)[1]\n",
    "    rated_items_by_user = np.array([i for i in range(number_of_items) if dataMat[user,i]>0])\n",
    "    test_size = test_ratio * len(rated_items_by_user)\n",
    "    test_indices = np.random.randint(0, len(rated_items_by_user), int(test_size))\n",
    "    withheld_items = rated_items_by_user[test_indices]\n",
    "    original_user_profile = np.copy(dataMat[user])\n",
    "    dataMat[user, withheld_items] = 0 # So that the withheld test items is not used in the rating estimation below\n",
    "    error_u = 0.0\n",
    "    count_u = len(withheld_items)\n",
    "\n",
    "    # Compute absolute error for user u over all test items\n",
    "    for item in withheld_items:\n",
    "        # Estimate rating on the withheld item\n",
    "        estimatedScore = estMethod(dataMat, user, simMeas, item)\n",
    "        error_u = error_u + abs(estimatedScore - original_user_profile[item])\n",
    "\n",
    "    # Now restore ratings of the withheld items to the user profile\n",
    "    for item in withheld_items:\n",
    "        dataMat[user, item] = original_user_profile[item]\n",
    "\n",
    "    # Return sum of absolute errors and the count of test cases for this user\n",
    "    # Note that these will have to be accumulated for each user to compute MAE\n",
    "    return error_u, count_u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Complete the definition for the function \"test\". This function iterates over all users and for each performs evaluation (by calling the provided \"cross_validate_user\" function), and returns the error information necessary to compute Mean Absolute Error (MAE). Use this function to perform evaluation (wiht 20% test-ratio for each user) comparing MAE results using standard item-based collaborative filtering (based on the rating prediction function \"standEst\") with results using the SVD-based version of the rating item-based CF (using \"svdEst\" as the prediction engine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataMat, test_ratio, estMethod):\n",
    "    # Write this function to iterate over all users and for each perform evaluation by calling\n",
    "    total_error = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for i in range(len(dataMat)):\n",
    "        error_u, count_u = cross_validate_user(dataMat, i, test_ratio, estMethod, simMeas=pearsSim)\n",
    "        total_error += error_u\n",
    "        total_count += count_u\n",
    "    # the above cross_validate_user function on each user. MAE will be the ratio of total error \n",
    "    # across all test cases to the total number of test cases, for all users\n",
    "    MAE = total_error / total_count\n",
    "    \n",
    "    print ('Mean Absoloute Error for ',estMethod,' : ',str(MAE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absoloute Error for  <function standEst at 0x7fbd35c72ca0>  :  3.725186962735859\n"
     ]
    }
   ],
   "source": [
    "mae_standEst = test(ratings, 0.2, standEst)\n",
    "mae_standEst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absoloute Error for  <function svdEst at 0x7fbd35c72b80>  :  3.635666770954332\n"
     ]
    }
   ],
   "source": [
    "mae_svdEst = test(ratings, 0.2, svdEst)\n",
    "mae_svdEst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Write a new function \"print_most_similar_jokes\" which takes the joke ratings data, a query joke id, a parameter k for the number of nearest neighbors, and a similarity metric function, and prints the text of the query joke as well as the texts of the top k most similar jokes based on user ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_similar_jokes(dataMat, jokes, queryJoke, k, metric=pearsSim):\n",
    "    # Write this function to find the k most similar jokes (based on user ratings) to a queryJoke\n",
    "    # The queryJoke is a joke id as given in the 'jokes.csv' file (an corresponding to the a column in dataMat)\n",
    "    # You must compare ratings for the queryJoke (the column in dataMat corresponding to the joke), to all\n",
    "    # other joke rating vectors and return the top k. Note that this is the same as performing KNN on the \n",
    "    # columns of dataMat. The function must retrieve the text of the joke from 'jokes.csv' file and print both\n",
    "    # the queryJoke text as well as the text of the returned jokes.\n",
    "    #Joke = dataMatT[queryJoke]\n",
    "    dataMatT = dataMat.T\n",
    "    temp = []\n",
    "    totalSim = np.zeros((len(dataMatT), 1))\n",
    "    sim = np.zeros((len(dataMatT), 1))\n",
    "                        \n",
    "    for i in range(len(dataMatT)):\n",
    "        similarity = metric(dataMatT[queryJoke], dataMatT[i])\n",
    "        if similarity == 1:\n",
    "            totalSim[i] = 0\n",
    "            sim[i] = i\n",
    "        else:\n",
    "            totalSim[i] = similarity\n",
    "            sim[i] = i\n",
    "\n",
    "    final = np.concatenate((totalSim, sim), axis =1)\n",
    "    sortedF = np.flip(final[final[:,0].argsort()])\n",
    "    Joke = sortedF[:, 0].astype(int)\n",
    "    \n",
    "    for j in Joke[:k]:\n",
    "        temp.append(j)\n",
    "    \n",
    "    print(\"Selected joke: \")\n",
    "    print(jokes[queryJoke] + '\\n')\n",
    "    print(\"Top %d Recommended jokes are :\" %k)\n",
    "    for i in temp:\n",
    "        print(jokes[i])\n",
    "        print(\"_______________\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected joke: \n",
      "Q. What's the difference between a man and a toilet? A. A toilet doesn't follow you around after you use it.\n",
      "\n",
      "Top 5 Recommended jokes are :\n",
      "What do you get when you run over a parakeet with a lawnmower? Shredded tweet.\n",
      "_______________\n",
      "A country guy goes into a city bar that has a dress code and the maitred' demands he wear a tie. Discouraged the guy goes to his car to sulk when inspiration strikes: He's got jumper cables in the trunk! So he wraps them around his neck sort of like a string tie (a bulky string tie to be sure) and returns to the bar. The maitre d' is reluctant but says to the guy \"Okay you're a pretty resourceful fellow you can come in... but just don't start anything\"!\n",
      "_______________\n",
      "Q. What's 200 feet long and has 4 teeth? A. The front row at a Willie Nelson Concert.\n",
      "_______________\n",
      "What do you call an American in the finals of the world cup?\"Hey Beer Man!\"\n",
      "_______________\n",
      "Q: What's the difference between a Lawyer and a Plumber? A: A Plumber works to unclog the system.\n",
      "_______________\n"
     ]
    }
   ],
   "source": [
    "print_most_similar_jokes(ratings, jokes, 3, 5, pearsSim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this problem you will use an image segmentation data set for clustering. You will experiment with using PCA as an approach to reduce dimensionality and noise in the data. You will compare the results of clustering the data with and without PCA using the provided image class assignments as the ground truth. The data set is divided into three files. The file \"segmentation_data.txt\" contains data about images with each line corresponding to one image. Each image is represented by 19 features (these are the columns in the data and correspond to the feature names in the file \"segmentation_names.txt\". The file \"segmentation_classes.txt\" contains the class labels (the type of image) and a numeric class label for each of the corresponding images in the data file. After clustering the image data, you will use the class labels to measure completeness and homogeneity of the generated clusters. The data set used in this problem is based on the Image Segmentation data set at the UCI Machine Learning Repository.\n",
    "\n",
    "### Your tasks in this problem are the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Load in the image data matrix (with rows as images and columns as features). Also load in the numeric class labels from the segmentation class file. Using your favorite method (e.g., sklearn's min-max scaler), perform min-max normalization on the data matrix so that each feature is scaled to [0,1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110.00</td>\n",
       "      <td>189.00</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.19</td>\n",
       "      <td>12.93</td>\n",
       "      <td>10.89</td>\n",
       "      <td>9.22</td>\n",
       "      <td>18.67</td>\n",
       "      <td>-6.11</td>\n",
       "      <td>-11.11</td>\n",
       "      <td>17.22</td>\n",
       "      <td>18.67</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.00</td>\n",
       "      <td>187.00</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.75</td>\n",
       "      <td>13.74</td>\n",
       "      <td>11.67</td>\n",
       "      <td>10.33</td>\n",
       "      <td>19.22</td>\n",
       "      <td>-6.22</td>\n",
       "      <td>-10.22</td>\n",
       "      <td>16.44</td>\n",
       "      <td>19.22</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>225.00</td>\n",
       "      <td>244.00</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.39</td>\n",
       "      <td>2.20</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.52</td>\n",
       "      <td>12.26</td>\n",
       "      <td>10.33</td>\n",
       "      <td>9.33</td>\n",
       "      <td>17.11</td>\n",
       "      <td>-5.78</td>\n",
       "      <td>-8.78</td>\n",
       "      <td>14.56</td>\n",
       "      <td>17.11</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.00</td>\n",
       "      <td>232.00</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.89</td>\n",
       "      <td>12.70</td>\n",
       "      <td>11.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>18.11</td>\n",
       "      <td>-5.11</td>\n",
       "      <td>-11.11</td>\n",
       "      <td>16.22</td>\n",
       "      <td>18.11</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97.00</td>\n",
       "      <td>186.00</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.01</td>\n",
       "      <td>15.59</td>\n",
       "      <td>13.89</td>\n",
       "      <td>11.78</td>\n",
       "      <td>21.11</td>\n",
       "      <td>-5.11</td>\n",
       "      <td>-11.44</td>\n",
       "      <td>16.56</td>\n",
       "      <td>21.11</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1   2    3    4    5    6    7    8     9     10    11    12  \\\n",
       "0 110.00 189.00   9 0.00 0.00 1.00 0.67 1.22 1.19 12.93 10.89  9.22 18.67   \n",
       "1  86.00 187.00   9 0.00 0.00 1.11 0.72 1.44 0.75 13.74 11.67 10.33 19.22   \n",
       "2 225.00 244.00   9 0.00 0.00 3.39 2.20 3.00 1.52 12.26 10.33  9.33 17.11   \n",
       "3  47.00 232.00   9 0.00 0.00 1.28 1.25 1.00 0.89 12.70 11.00  9.00 18.11   \n",
       "4  97.00 186.00   9 0.00 0.00 1.17 0.69 1.17 1.01 15.59 13.89 11.78 21.11   \n",
       "\n",
       "     13     14    15    16   17   18  \n",
       "0 -6.11 -11.11 17.22 18.67 0.51 1.91  \n",
       "1 -6.22 -10.22 16.44 19.22 0.46 1.94  \n",
       "2 -5.78  -8.78 14.56 17.11 0.48 1.99  \n",
       "3 -5.11 -11.11 16.22 18.11 0.50 1.88  \n",
       "4 -5.11 -11.44 16.56 21.11 0.44 1.86  "
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in the image data matrix (with rows as images and columns as features).\n",
    "import pandas as pd\n",
    "data = pd.read_table(path + '/segmentation_data/segmentation_data.txt' ,sep=',',header = None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRASS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1\n",
       "0  GRASS  0\n",
       "1  GRASS  0\n",
       "2  GRASS  0\n",
       "3  GRASS  0\n",
       "4  GRASS  0"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Also load in the numeric class labels from the segmentation class file.\n",
    "labels = pd.read_table(path + '/segmentation_data/segmentation_classes.txt', header = None)\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['REGION-CENTROID-COL',\n",
       " 'REGION-CENTROID-ROW',\n",
       " 'REGION-PIXEL-COUNT',\n",
       " 'SHORT-LINE-DENSITY-5',\n",
       " 'SHORT-LINE-DENSITY-2',\n",
       " 'VEDGE-MEAN',\n",
       " 'VEDGE-SD',\n",
       " 'HEDGE-MEAN',\n",
       " 'HEDGE-SD',\n",
       " 'INTENSITY-MEAN',\n",
       " 'RAWRED-MEAN',\n",
       " 'RAWBLUE-MEAN',\n",
       " 'RAWGREEN-MEAN',\n",
       " 'EXRED-MEAN',\n",
       " 'EXBLUE-MEAN',\n",
       " 'EXGREEN-MEAN',\n",
       " 'VALUE-MEAN',\n",
       " 'SATURATION-MEAN',\n",
       " 'HUE-MEAN']"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = pd.read_table(path + '/segmentation_data/segmentation_names.txt', header = None)\n",
    "names = list(names[0])\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.30830040e-01, 7.41666667e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.42205474e-02, 6.72233922e-04, 2.73291926e-02,\n",
       "        8.55743510e-04, 9.01110284e-02, 7.94165331e-02, 6.11192912e-02,\n",
       "        1.30943107e-01, 7.31343290e-01, 1.41176540e-02, 8.72865267e-01,\n",
       "        1.23711348e-01, 5.08138840e-01, 8.31849232e-01],\n",
       "       [3.35968379e-01, 7.33333333e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.80228046e-02, 7.26095734e-04, 3.22981359e-02,\n",
       "        5.41219947e-04, 9.57913810e-02, 8.50891441e-02, 6.84830672e-02,\n",
       "        1.34840205e-01, 7.29477615e-01, 2.35294199e-02, 8.59582565e-01,\n",
       "        1.27393216e-01, 4.63329080e-01, 8.36986460e-01],\n",
       "       [8.85375494e-01, 9.70833333e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.15969577e-01, 2.21344355e-03, 6.70807367e-02,\n",
       "        1.09658970e-03, 8.54634659e-02, 7.53646732e-02, 6.18556741e-02,\n",
       "        1.20031165e-01, 7.36940304e-01, 3.88235327e-02, 8.27324481e-01,\n",
       "        1.13402054e-01, 4.80149030e-01, 8.44782328e-01],\n",
       "       [1.81818182e-01, 9.20833333e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.37262383e-02, 1.26509804e-03, 2.23602471e-02,\n",
       "        6.45176713e-04, 8.85618432e-02, 8.02269050e-02, 5.96465386e-02,\n",
       "        1.27045974e-01, 7.48134334e-01, 1.41176540e-02, 8.55787468e-01,\n",
       "        1.20029447e-01, 5.00965950e-01, 8.25889142e-01],\n",
       "       [3.79446640e-01, 7.29166667e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.99239709e-02, 6.96986866e-04, 2.60869646e-02,\n",
       "        7.25325858e-04, 1.08701264e-01, 1.01296598e-01, 7.80559656e-02,\n",
       "        1.48090401e-01, 7.48134334e-01, 1.05882352e-02, 8.61480079e-01,\n",
       "        1.39911626e-01, 4.42660570e-01, 8.23923576e-01]])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using your favorite method (e.g., sklearn's min-max scaler), perform min-max normalization on the data matrix so \n",
    "#that each feature is scaled to [0,1] range.\n",
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler().fit(data)\n",
    "data_n = min_max_scaler.transform(data)\n",
    "data_n[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Next, Perform Kmeans clustering (for this problem, use the Kmeans implementation in scikit-learn) on the image data (since there are a total 7 pre-assigned image classes, you should use K = 7 in your clustering). Use Euclidean distance as your distance measure for the clustering. Print the cluster centroids (use some formatting so that they are visually understandable). Compare your 7 clusters to the 7 pre-assigned classes by computing the Completeness and Homogeneity values of the generated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration 0, inertia 687.3919646585911\n",
      "Iteration 1, inertia 390.2340074923927\n",
      "Iteration 2, inertia 380.2009046272517\n",
      "Iteration 3, inertia 374.91656856941404\n",
      "Iteration 4, inertia 371.7750871048173\n",
      "Iteration 5, inertia 370.21319634906206\n",
      "Iteration 6, inertia 369.0569608466499\n",
      "Iteration 7, inertia 368.37732886676775\n",
      "Iteration 8, inertia 367.9492839743955\n",
      "Iteration 9, inertia 367.74335080648626\n",
      "Iteration 10, inertia 367.64264004709867\n",
      "Iteration 11, inertia 367.5857048730359\n",
      "Iteration 12, inertia 367.5783347765411\n",
      "Iteration 13, inertia 367.5713177682432\n",
      "Iteration 14, inertia 367.56621325336096\n",
      "Iteration 15, inertia 367.5597457933926\n",
      "Converged at iteration 15: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 602.9575095972283\n",
      "Iteration 1, inertia 397.9005447102638\n",
      "Iteration 2, inertia 387.89488636428524\n",
      "Iteration 3, inertia 380.18405634863933\n",
      "Iteration 4, inertia 374.1412269284418\n",
      "Iteration 5, inertia 370.3921940499043\n",
      "Iteration 6, inertia 369.17526523087093\n",
      "Iteration 7, inertia 368.7011426744532\n",
      "Iteration 8, inertia 368.5049623744413\n",
      "Iteration 9, inertia 368.3712411235171\n",
      "Iteration 10, inertia 368.24464522103375\n",
      "Iteration 11, inertia 368.0699856548697\n",
      "Iteration 12, inertia 368.0084259072\n",
      "Iteration 13, inertia 367.87142643676765\n",
      "Iteration 14, inertia 367.74963062359365\n",
      "Iteration 15, inertia 367.66903847192077\n",
      "Iteration 16, inertia 367.6404848792903\n",
      "Iteration 17, inertia 367.6276717092612\n",
      "Iteration 18, inertia 367.6259711840506\n",
      "Converged at iteration 18: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 498.70042460214404\n",
      "Iteration 1, inertia 376.0169569077844\n",
      "Iteration 2, inertia 372.4675146831516\n",
      "Iteration 3, inertia 371.6318225491917\n",
      "Iteration 4, inertia 371.1535899604468\n",
      "Iteration 5, inertia 370.9424067750604\n",
      "Iteration 6, inertia 370.79524363710686\n",
      "Iteration 7, inertia 370.6683456816851\n",
      "Iteration 8, inertia 370.5091493063363\n",
      "Iteration 9, inertia 370.385226673862\n",
      "Iteration 10, inertia 370.2802693085856\n",
      "Iteration 11, inertia 370.15029639165886\n",
      "Iteration 12, inertia 370.0696646633133\n",
      "Iteration 13, inertia 369.91918417241953\n",
      "Iteration 14, inertia 369.76744650109293\n",
      "Iteration 15, inertia 369.4735023631584\n",
      "Iteration 16, inertia 368.9684101136705\n",
      "Iteration 17, inertia 368.23821119594356\n",
      "Iteration 18, inertia 366.45677417499473\n",
      "Iteration 19, inertia 363.09385971338475\n",
      "Iteration 20, inertia 357.6152066964588\n",
      "Iteration 21, inertia 352.56983521936945\n",
      "Iteration 22, inertia 350.696138826563\n",
      "Iteration 23, inertia 350.1559102194987\n",
      "Iteration 24, inertia 350.0411586747133\n",
      "Iteration 25, inertia 350.0135491179548\n",
      "Iteration 26, inertia 350.0120797410692\n",
      "Converged at iteration 26: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 521.5082277383009\n",
      "Iteration 1, inertia 382.66411649801097\n",
      "Iteration 2, inertia 372.95069717674824\n",
      "Iteration 3, inertia 365.2077507966995\n",
      "Iteration 4, inertia 361.4653365104148\n",
      "Iteration 5, inertia 356.1186137524455\n",
      "Iteration 6, inertia 351.8125896357877\n",
      "Iteration 7, inertia 350.46142125676977\n",
      "Iteration 8, inertia 350.09455423935856\n",
      "Iteration 9, inertia 350.0310527457603\n",
      "Iteration 10, inertia 350.0138602138511\n",
      "Iteration 11, inertia 350.0120797410693\n",
      "Converged at iteration 11: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 555.257812206274\n",
      "Iteration 1, inertia 375.4170535930517\n",
      "Iteration 2, inertia 372.23657033936104\n",
      "Iteration 3, inertia 371.7403141750385\n",
      "Iteration 4, inertia 371.5990799687849\n",
      "Iteration 5, inertia 371.5212781788181\n",
      "Iteration 6, inertia 371.3973338999263\n",
      "Iteration 7, inertia 371.1912551737099\n",
      "Iteration 8, inertia 370.7724615018051\n",
      "Iteration 9, inertia 370.1027729622369\n",
      "Iteration 10, inertia 369.9029622715465\n",
      "Iteration 11, inertia 369.86803167467497\n",
      "Iteration 12, inertia 369.8620874994306\n",
      "Converged at iteration 12: center shift 3.369030555427534e-06 within tolerance 4.150156953284628e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 521.7124296006609\n",
      "Iteration 1, inertia 383.69304003274374\n",
      "Iteration 2, inertia 380.60292430221585\n",
      "Iteration 3, inertia 379.80913278154253\n",
      "Iteration 4, inertia 379.11420717461243\n",
      "Iteration 5, inertia 378.24541852041153\n",
      "Iteration 6, inertia 377.4377547295539\n",
      "Iteration 7, inertia 376.59146623996645\n",
      "Iteration 8, inertia 375.1205466849094\n",
      "Iteration 9, inertia 373.02428800270224\n",
      "Iteration 10, inertia 372.2376719926519\n",
      "Iteration 11, inertia 372.0516514393031\n",
      "Iteration 12, inertia 371.8377625001698\n",
      "Iteration 13, inertia 371.6238035727682\n",
      "Iteration 14, inertia 371.51929856922783\n",
      "Iteration 15, inertia 371.45855402637704\n",
      "Iteration 16, inertia 371.3777450428296\n",
      "Iteration 17, inertia 371.2731611692903\n",
      "Iteration 18, inertia 371.20451643290505\n",
      "Iteration 19, inertia 371.07111316996424\n",
      "Iteration 20, inertia 371.02400073645856\n",
      "Iteration 21, inertia 371.0053852289336\n",
      "Iteration 22, inertia 370.9944004040403\n",
      "Iteration 23, inertia 370.9862182169484\n",
      "Iteration 24, inertia 370.9715748795779\n",
      "Iteration 25, inertia 370.96734185108664\n",
      "Iteration 26, inertia 370.9651628719978\n",
      "Converged at iteration 26: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 576.5937547623778\n",
      "Iteration 1, inertia 397.990706737878\n",
      "Iteration 2, inertia 390.09482300874726\n",
      "Iteration 3, inertia 385.35049139013626\n",
      "Iteration 4, inertia 381.7500536583874\n",
      "Iteration 5, inertia 379.0155204402367\n",
      "Iteration 6, inertia 376.797681826811\n",
      "Iteration 7, inertia 375.3004444397407\n",
      "Iteration 8, inertia 374.5418384552838\n",
      "Iteration 9, inertia 374.2025550102562\n",
      "Iteration 10, inertia 374.0457304702454\n",
      "Iteration 11, inertia 373.98015084878585\n",
      "Iteration 12, inertia 373.97037456698473\n",
      "Iteration 13, inertia 373.9616610794273\n",
      "Iteration 14, inertia 373.9548864436455\n",
      "Iteration 15, inertia 373.9499510448776\n",
      "Converged at iteration 15: center shift 1.336396525883386e-06 within tolerance 4.150156953284628e-06.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 556.2574580622723\n",
      "Iteration 1, inertia 382.36710387335273\n",
      "Iteration 2, inertia 370.19528407423593\n",
      "Iteration 3, inertia 362.1614845661022\n",
      "Iteration 4, inertia 356.72190086836736\n",
      "Iteration 5, inertia 353.4537560905659\n",
      "Iteration 6, inertia 352.17048326833066\n",
      "Iteration 7, inertia 351.7646293104192\n",
      "Iteration 8, inertia 351.47339768017207\n",
      "Iteration 9, inertia 351.3602229240148\n",
      "Iteration 10, inertia 351.2986804712076\n",
      "Iteration 11, inertia 351.2650149221562\n",
      "Iteration 12, inertia 351.2291553219893\n",
      "Iteration 13, inertia 351.1372559070882\n",
      "Iteration 14, inertia 350.9103751739907\n",
      "Iteration 15, inertia 350.49036576862466\n",
      "Iteration 16, inertia 350.10506879105816\n",
      "Iteration 17, inertia 350.04480434637856\n",
      "Iteration 18, inertia 350.02667200885617\n",
      "Iteration 19, inertia 350.02186147714303\n",
      "Iteration 20, inertia 350.01547472608297\n",
      "Iteration 21, inertia 350.012902719455\n",
      "Converged at iteration 21: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 568.8188158366028\n",
      "Iteration 1, inertia 384.28068779471755\n",
      "Iteration 2, inertia 374.4462530689088\n",
      "Iteration 3, inertia 370.91673160698315\n",
      "Iteration 4, inertia 367.8989644755646\n",
      "Iteration 5, inertia 365.4717851341369\n",
      "Iteration 6, inertia 364.7744792970373\n",
      "Iteration 7, inertia 364.4943004086873\n",
      "Iteration 8, inertia 364.2234335876406\n",
      "Iteration 9, inertia 363.5997395581052\n",
      "Iteration 10, inertia 361.30055885235737\n",
      "Iteration 11, inertia 355.4607466801232\n",
      "Iteration 12, inertia 351.74115861959456\n",
      "Iteration 13, inertia 350.412504244022\n",
      "Iteration 14, inertia 350.0857232312433\n",
      "Iteration 15, inertia 350.02848849052754\n",
      "Iteration 16, inertia 350.0120797410692\n",
      "Converged at iteration 16: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 486.56065736005394\n",
      "Iteration 1, inertia 381.99106716620236\n",
      "Iteration 2, inertia 379.5667618081343\n",
      "Iteration 3, inertia 378.61464211022155\n",
      "Iteration 4, inertia 377.8521194173391\n",
      "Iteration 5, inertia 377.36431622507956\n",
      "Iteration 6, inertia 377.1675172347684\n",
      "Iteration 7, inertia 377.06681654624447\n",
      "Iteration 8, inertia 377.00227668468546\n",
      "Iteration 9, inertia 376.97111429086084\n",
      "Iteration 10, inertia 376.96448605867744\n",
      "Iteration 11, inertia 376.9558456629598\n",
      "Iteration 12, inertia 376.9225219160462\n",
      "Iteration 13, inertia 376.88889962842387\n",
      "Iteration 14, inertia 376.8623623573162\n",
      "Iteration 15, inertia 376.78577851459494\n",
      "Iteration 16, inertia 376.72036981431546\n",
      "Iteration 17, inertia 376.6864216346683\n",
      "Iteration 18, inertia 376.6733650180756\n",
      "Converged at iteration 18: center shift 1.5113570129055307e-06 within tolerance 4.150156953284628e-06.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(max_iter=500, n_clusters=7, verbose=1)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform Kmeans clustering (for this problem, use the Kmeans implementation in scikit-learn) on the image data \n",
    "#(since there are a total 7 pre-assigned image classes, you should use K = 7 in your clustering). \n",
    "from sklearn.cluster import KMeans\n",
    "kmean = KMeans(n_clusters = 7, max_iter=500, verbose=1)\n",
    "kmean.fit(data_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Euclidean distance as your distance measure for the clustering. Print the cluster centroids \n",
    "#(use some formatting so that they are visually understandable).\n",
    "\n",
    "clusters = kmean.predict(data_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REGION-CENTROID-COL</th>\n",
       "      <th>REGION-CENTROID-ROW</th>\n",
       "      <th>REGION-PIXEL-COUNT</th>\n",
       "      <th>SHORT-LINE-DENSITY-5</th>\n",
       "      <th>SHORT-LINE-DENSITY-2</th>\n",
       "      <th>VEDGE-MEAN</th>\n",
       "      <th>VEDGE-SD</th>\n",
       "      <th>HEDGE-MEAN</th>\n",
       "      <th>HEDGE-SD</th>\n",
       "      <th>INTENSITY-MEAN</th>\n",
       "      <th>RAWRED-MEAN</th>\n",
       "      <th>RAWBLUE-MEAN</th>\n",
       "      <th>RAWGREEN-MEAN</th>\n",
       "      <th>EXRED-MEAN</th>\n",
       "      <th>EXBLUE-MEAN</th>\n",
       "      <th>EXGREEN-MEAN</th>\n",
       "      <th>VALUE-MEAN</th>\n",
       "      <th>SATURATION-MEAN</th>\n",
       "      <th>HUE-MEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   REGION-CENTROID-COL  REGION-CENTROID-ROW  REGION-PIXEL-COUNT  \\\n",
       "0                 0.25                 0.46                0.00   \n",
       "1                 0.54                 0.15                0.00   \n",
       "2                 0.26                 0.39                0.00   \n",
       "3                 0.51                 0.81                0.00   \n",
       "4                 0.77                 0.43                0.00   \n",
       "5                 0.30                 0.53                0.00   \n",
       "6                 0.75                 0.53                0.00   \n",
       "\n",
       "   SHORT-LINE-DENSITY-5  SHORT-LINE-DENSITY-2  VEDGE-MEAN  VEDGE-SD  \\\n",
       "0                  0.03                  0.01        0.04      0.00   \n",
       "1                  0.03                  0.00        0.03      0.00   \n",
       "2                  0.07                  0.02        0.08      0.00   \n",
       "3                  0.08                  0.01        0.05      0.00   \n",
       "4                  0.01                  0.02        0.04      0.00   \n",
       "5                  0.05                  0.05        0.10      0.01   \n",
       "6                  0.04                  0.04        0.11      0.02   \n",
       "\n",
       "   HEDGE-MEAN  HEDGE-SD  INTENSITY-MEAN  RAWRED-MEAN  RAWBLUE-MEAN  \\\n",
       "0        0.03      0.00            0.03         0.02          0.04   \n",
       "1        0.03      0.00            0.82         0.78          0.89   \n",
       "2        0.06      0.00            0.15         0.14          0.19   \n",
       "3        0.05      0.00            0.11         0.09          0.09   \n",
       "4        0.02      0.00            0.04         0.04          0.06   \n",
       "5        0.08      0.01            0.40         0.37          0.47   \n",
       "6        0.11      0.02            0.30         0.28          0.35   \n",
       "\n",
       "   RAWGREEN-MEAN  EXRED-MEAN  EXBLUE-MEAN  EXGREEN-MEAN  VALUE-MEAN  \\\n",
       "0           0.02        0.77         0.22          0.51        0.04   \n",
       "1           0.79        0.27         0.67          0.29        0.89   \n",
       "2           0.12        0.72         0.34          0.36        0.19   \n",
       "3           0.14        0.68         0.08          0.82        0.13   \n",
       "4           0.03        0.78         0.22          0.49        0.06   \n",
       "5           0.35        0.50         0.57          0.21        0.47   \n",
       "6           0.27        0.59         0.45          0.31        0.35   \n",
       "\n",
       "   SATURATION-MEAN  HUE-MEAN  \n",
       "0             0.80      0.18  \n",
       "1             0.21      0.13  \n",
       "2             0.41      0.20  \n",
       "3             0.41      0.89  \n",
       "4             0.54      0.24  \n",
       "5             0.30      0.16  \n",
       "6             0.30      0.16  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format='{:,.2f}'.format\n",
    "\n",
    "centroids = pd.DataFrame(kmean.cluster_centers_, columns = names)\n",
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 4, 4, 2], dtype=int32)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Completeness of Clusters: 0.613187012485301\n",
      "The homogeneity of Clusters : 0.6115021163370862\n"
     ]
    }
   ],
   "source": [
    "#Compare your 7 clusters to the 7 pre-assigned classes by computing the Completeness and \n",
    "#Homogeneity values of the generated clusters.\n",
    "from sklearn.metrics import completeness_score, homogeneity_score\n",
    "\n",
    "completeness = completeness_score(labels[1], clusters)\n",
    "homoScore = homogeneity_score(labels[1],clusters)\n",
    "\n",
    "print('The Completeness of Clusters: '+  str(completeness))\n",
    "print ('The homogeneity of Clusters : ' + str(homoScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Perform PCA on the normalized image data matrix. You may use the linear algebra package in Numpy or the Decomposition module in scikit-learn (the latter is much more efficient). Analyze the principal components to determine the number, r, of PCs needed to capture at least 95% of variance in the data. Then use these r components as features to transform the data into a reduced dimension space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "pca = decomposition.PCA(n_components = 6)\n",
    "DTtrans = pca.fit(data_n).transform(data_n)\n",
    "np.set_printoptions(precision=2,suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61 0.13 0.1  0.05 0.04 0.02]\n",
      "variance in the data:  0.9411392197960624\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)\n",
    "print ('variance in the data: ', pca.explained_variance_ratio_.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components = 7)\n",
    "DTtrans = pca.fit(data_n).transform(data_n)\n",
    "np.set_printoptions(precision=2,suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61 0.13 0.1  0.05 0.04 0.02 0.02]\n",
      "variance in the data:  0.9600589227704956\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)\n",
    "print ('variance in the data: ', pca.explained_variance_ratio_.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#therefore we chose n = 7 for at least 95% of variance in the data which we got 96%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Perform Kmeans again, but this time on the lower dimensional transformed data. Then, compute the Completeness and Homogeneity values of the new clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=7)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmean = KMeans(n_clusters = 7)\n",
    "kmean.fit(DTtrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 ... 6 6 4]\n"
     ]
    }
   ],
   "source": [
    "print(kmean.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster 1</th>\n",
       "      <th>Cluster 2</th>\n",
       "      <th>Cluster 3</th>\n",
       "      <th>Cluster 4</th>\n",
       "      <th>Cluster 5</th>\n",
       "      <th>Cluster 6</th>\n",
       "      <th>Cluster 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PC 1</th>\n",
       "      <td>-0.60</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC 2</th>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC 3</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC 4</th>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC 5</th>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC 6</th>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Cluster 1  Cluster 2  Cluster 3  Cluster 4  Cluster 5   Cluster 6  \\\n",
       "PC 1      -0.60       1.41       0.18      -0.62      -0.20        0.44   \n",
       "PC 2      -0.36       0.09       0.05       0.64      -0.24       -0.11   \n",
       "PC 3       0.11       0.04      -0.27       0.20       0.15        0.17   \n",
       "PC 4      -0.13      -0.17       0.19      -0.09       0.06        0.24   \n",
       "PC 5      -0.13      -0.03       0.03      -0.06       0.13       -0.05   \n",
       "PC 6      -0.02      -0.01       0.02       0.01      -0.01       -0.01   \n",
       "\n",
       "      Cluster 7  \n",
       "PC 1      -0.51  \n",
       "PC 2      -0.07  \n",
       "PC 3      -0.34  \n",
       "PC 4      -0.07  \n",
       "PC 5       0.07  \n",
       "PC 6       0.01  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(kmean.cluster_centers_.T, index = ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5', 'PC 6'], columns = ['Cluster 1', 'Cluster 2','Cluster 3','Cluster 4','Cluster 5',' Cluster 6','Cluster 7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Completeness of Clusters: 0.6096968341816197\n",
      "The homogeneity of Clusters : 0.6080039254811925\n"
     ]
    }
   ],
   "source": [
    "completeness = completeness_score(labels[1], kmean.labels_)\n",
    "homoScore = homogeneity_score(labels[1],kmean.labels_)\n",
    "\n",
    "print('The Completeness of Clusters: '+  str(completeness))\n",
    "print ('The homogeneity of Clusters : ' + str(homoScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Discuss your observations based on the comparison of the two clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without PCA:\n",
    "#The Completeness of Clusters: 0.613187012485301\n",
    "#The homogeneity of Clusters : 0.6115021163370862\n",
    "\n",
    "#With PCA dimension reduction:\n",
    "#The Completeness of Clusters: 0.6096968341816197\n",
    "#The homogeneity of Clusters : 0.6080039254811925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By comparing these two sets of results, we can see they are very similar between with PCA\n",
    "#and without PCA dimension reduction. The scores for both completeness and homoheneity \n",
    "#are slightl better with the lower dimension transformed data. Using PCA to find the number of \n",
    "#perinciple components and then use those components to transform the data into a lower dimensionl \n",
    "#space, the results were actually impoved little."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
